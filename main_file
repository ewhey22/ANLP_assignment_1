#Here are some libraries you're likely to use. You might want/need others as well.
import re
import sys
from random import random
from random import choices
import random
from math import log
from collections import defaultdict
from itertools import product
import numpy as np
import matplotlib.pyplot as plt

def process(line):
    """
    Takes a line of text and preprocesses it according to instrucs in task
    :param line:
    :return: processed line
    """
    x = re.sub(r'[^A-Za-z0-9\s\.]+', "", line)
    x = re.sub(r'\n', '', x)
    # convert all digits to 0
    x = re.sub(r'\d', "0", x)

    # add start of sequence and end of sequence character '#' to each sequence
    x = f"##{x}#"

    return x.lower()

def splits_file(file):
    """
    Takes an input file and splits it into training and validation docs.
    :param file:
    :return: a tuple with the first list a list of lines in training and the second a list of lines in validation.
    """
    with open(file, 'r') as f:
        lines = f.readlines()
        random.shuffle(lines)

    split = int(0.8*len(lines))
    train_lines = lines[:split]
    val_lines = lines[split:]
    return train_lines, val_lines

def create_count_dict():
    char = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',' ','.', '0']
    char_hash = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',' ','.', '0','#']
    first_bigrams = [''.join(bigram) for bigram in product(char, char)]
    all_tri_counts = dict.fromkeys(first_bigrams)
    for i in first_bigrams:
        all_tri_counts[i] = dict.fromkeys(char_hash,0)
    all_tri_counts['##'] = dict.fromkeys(char,0)
    #Be careful here, we do not want to allow #g#!!!!
    for j in char:
        all_tri_counts[f'#{j}'] = dict.fromkeys(char,0)
    return all_tri_counts

def true_count_dict(list_of_lines):
    """
        Takes a file and returns a dictionary of dictionary of trigrams with the count of each trigram
        :param file:
        :return: dictionary of dictionary of trigrams with the count of each trigram
        """

    #First we count for all the trigrams that occur in the input file
    tri_counts = defaultdict(int)  # counts of all trigrams in input
    for line in list_of_lines:
        line = process(line)
        for j in range(len(line) - (2)):
            trigram = line[j:j + 3]
            tri_counts[trigram] += 1

    #Now we create our dictionary with all possible trigrams and add in the counts that we just found above
    dd = create_count_dict()
    for i in tri_counts.keys():
        dd[i[0:2]][i[2]] = tri_counts[i]

    return dd



## Now modify the counts using add-alpha

def smoothed_probs(dd, alpha):
    """
    Takes a dictionary of counts and uses add-alpha smoothing (same alpha for each context) to get smoothed probs
    :param dd: dictionary of dictionaries with counts of each trigram
    :param alpha: smoothing hyperparamter
    :return: smoothed counts dictionary
    """
    #First modify counts
    for i in dd.keys():
        for j in dd[i].keys():
            dd[i][j] += alpha

    #Be careful when normalising as not all histories have the same possible third characters (### is not allowed etc)
    for i in dd.keys():
        tot_count = sum(dd[i].values())
        for j in dd[i].keys():
            dd[i][j] = dd[i][j] / tot_count

    return dd

def read_txt_file(filename):
    """
    This function takes a txt file in the format of model-br.en and outputs a dictionary of dictionaries
    so that each probability is one of the values of the nested dictionary.
    :param filename
    :return: dictionary of dictionaries
    """

    with open(filename, 'r') as f:
        lines = f.readlines()
        keys = sorted(list(set([y[0:2] for y in lines])))
        prob_dict = {i: {} for i in keys}
        for j in lines:
            req_dic = prob_dict[j[:2]]
            req_dic[j[2]] = float(j[3:])
    return prob_dict

def generate_string(model, no_char):
    """
    Takes the model as dictionary of dictionaries and generates a random string of characters.
    :param model:
    :param no_char (number of characters in string)
    :return:
    """
    string = "##"
    for i in range(no_char):
        if string[-1] == "#" and string [-2] != "#":
            string = string + "\n##"
        else:
            string = string

        new_dict = model[string[-2:]]
        new_char = choices(list(new_dict.keys()), list(new_dict.values()))
        string = string + new_char[0]
    return string

def perplexity(doc, model):
    """
    Function takes in a test document (WHICH HAS NOT BEEN PREPROCESSED) and a model (of the form dictionary of dictionaries.
    It outputs the perplexity of the document under this model
    """

    if type(doc) == list:
        logsum = 0
        char_count = 0
        for line in doc:
            line = process(line)
            for i in range(len(line) - 2):
                dic = model[line[i:i + 2]]
                prob = float(dic[line[i + 2]])
                logsum += np.log2(prob)
                char_count += 1
        cross_ent = (-1 / char_count) * logsum

    else:
        with open(doc) as f:
            logsum = 0
            char_count = 0
            for line in f:
                line = process(line)
                for i in range(len(line)-2):
                    dic = model[line[i:i+2]]
                    prob = float(dic[line[i+2]])
                    logsum += np.log2(prob)
                    char_count += 1
            cross_ent = (-1/char_count)*logsum
    return 2**cross_ent


def loop_alphas(list_of_lines_train, list_of_lines_val, list_of_alphas):
    perplex = {}
    for aa in list_of_alphas:
        dd = true_count_dict(list_of_lines_train)
        mod = smoothed_probs(dd, aa)
        perp = perplexity(list_of_lines_val, mod)
        perplex[aa] = perp
    return perplex


def modeller(file):
    train,val = splits_file(file)
    hh = loop_alphas(train, val, np.arange(0.0001, 1, 0.05))
    alpha = min(hh, key=hh.get)
    dd = true_count_dict(train)
    mod = smoothed_probs(dd, alpha)
    return mod


eng_model = modeller("training.en")
ger_model = modeller("training.de")
esp_model = modeller("training.es")

#print(generate_string(eng_model, 300))
#print('------------------------------------------------')
#print(generate_string(read_txt_file('model-br.en'),300))


print(perplexity("test", eng_model))
print(perplexity("test", ger_model))
print(perplexity("test", esp_model))


